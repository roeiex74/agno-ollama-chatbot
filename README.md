# Agno + Ollama Full-Stack Chatbot ü§ñ

> **‚ö° AI-Powered Development Notice**: This entire project was developed using **Claude Code** (Anthropic's AI coding assistant). All code, architecture decisions, implementation details, and documentation were generated through AI-assisted development, demonstrating the power and capabilities of modern LLM-based development tools in creating production-ready applications.

A production-ready, full-stack chatbot application powered by **Agno** (agent framework) and **Ollama** (local LLM inference) with a modern React frontend. Features real-time streaming responses, persistent conversation history via PostgreSQL (Neon), and a beautiful ChatGPT-inspired interface.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![Node](https://img.shields.io/badge/node-20+-green.svg)
![TypeScript](https://img.shields.io/badge/typescript-5.0+-blue.svg)
![FastAPI](https://img.shields.io/badge/fastapi-0.115+-green.svg)
![React](https://img.shields.io/badge/react-19+-blue.svg)

## üìã Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Prerequisites](#-prerequisites)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Configuration](#-configuration)
- [API Documentation](#-api-documentation)
- [Development](#-development)
- [Deployment](#-deployment)
- [Testing](#-testing)
- [Troubleshooting](#-troubleshooting)
- [AI Development](#-ai-development-journey)
- [Contributing](#-contributing)
- [License](#-license)

## ‚ú® Features

### Core Functionality
- üí¨ **Real-time Streaming Chat**: Server-Sent Events (SSE) for token-by-token streaming responses
- üîÑ **Conversation Management**: Full CRUD operations (create, read, update, delete) for chat conversations
- üíæ **Persistent Storage**: PostgreSQL database integration via Neon for reliable conversation history
- üéØ **Session Management**: Automatic session creation and restoration using Agno's session API
- üöÄ **Local LLM**: Privacy-first approach using Ollama for local inference (no API keys required)
- üé® **Auto-Title Generation**: Conversations automatically titled from first user message

### User Experience
- üé≠ **Modern UI**: ChatGPT-inspired interface with dark mode support and smooth animations
- ‚ö° **Optimistic Updates**: Instant UI feedback with background synchronization
- üì± **Responsive Design**: Mobile-first design that works seamlessly on all screen sizes
- üîç **Conversation History**: Browse and restore previous conversations with one click
- üìù **Smart Message Filtering**: System messages filtered from display for cleaner conversations
- ‚è±Ô∏è **Relative Timestamps**: Human-readable time displays (e.g., "5 min ago", "2 hr ago")
- üé¨ **Loading States**: Visual feedback during all async operations
- üõë **Stream Cancellation**: Stop button to cancel ongoing streaming responses
- üìÑ **Markdown Rendering**: Full GitHub Flavored Markdown support with syntax highlighting
- üé® **Memoized Rendering**: Performance-optimized markdown with block-level memoization for smooth streaming
- üìã **Copy Functionality**: One-click copy button for assistant messages

### Technical Features
- üèóÔ∏è **Type-Safe**: Full TypeScript implementation on frontend with strict type checking
- üîê **Error Handling**: Comprehensive error boundaries and user-friendly error messages
- üéõÔ∏è **State Management**: Redux Toolkit with RTK Query for efficient data fetching and caching
- üîÑ **Hot Reload**: Fast development with Vite HMR and Uvicorn auto-reload
- üìä **Real-time Refetching**: Conversations refetch from server on every selection for data consistency
- üé≠ **Serializable State**: All Redux state uses serializable data types (number timestamps)
- üîå **API Integration**: Clean separation of API layer using RTK Query
- üé® **Component Library**: Built with shadcn/ui for consistent, accessible components

## üèóÔ∏è Architecture

### System Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Frontend (React 18 + Vite)                        ‚îÇ
‚îÇ                                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ  Components  ‚îÇ  ‚îÇ Redux Store  ‚îÇ  ‚îÇ  RTK Query   ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  (shadcn/ui) ‚îÇ‚óÑ‚îÄ‚î§   (State)    ‚îÇ‚óÑ‚îÄ‚î§   (API)      ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ - ChatLayout ‚îÇ  ‚îÇ - Conversa-  ‚îÇ  ‚îÇ - chatApi    ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ - ChatArea   ‚îÇ  ‚îÇ   tions      ‚îÇ  ‚îÇ - conversa-  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ - ChatInput  ‚îÇ  ‚îÇ - UI State   ‚îÇ  ‚îÇ   tionsApi   ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ - Message    ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                 ‚îÇ
                                      HTTP/SSE   ‚îÇ
                                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                      ‚îÇ   CORS Enabled    ‚îÇ
                                      ‚îÇ   API Gateway     ‚îÇ
                                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Backend (FastAPI + Uvicorn)                      ‚îÇ
‚îÇ                                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ   Endpoints  ‚îÇ‚îÄ‚ñ∫‚îÇ ChatbotAgent ‚îÇ‚îÄ‚ñ∫‚îÇ Ollama Model ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  (REST+SSE)  ‚îÇ  ‚îÇ    (Agno)    ‚îÇ  ‚îÇ   (Local)    ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ /healthz     ‚îÇ  ‚îÇ - Session    ‚îÇ  ‚îÇ - llama3.2   ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ /chat        ‚îÇ  ‚îÇ   mgmt       ‚îÇ  ‚îÇ - gpt-oss    ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ /chat/stream ‚îÇ  ‚îÇ - History    ‚îÇ  ‚îÇ - Custom     ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ /conversations‚îÇ ‚îÇ   loading    ‚îÇ  ‚îÇ   models     ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                     Agno PostgresDb
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PostgreSQL Database (Neon - Serverless)                 ‚îÇ
‚îÇ                                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                  agno_sessions Table                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ session_id (VARCHAR, PK) - Unique conversation ID         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ session_type (VARCHAR) - "agent" for our use case         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ session_data (JSONB) - Metadata including custom title    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ agent_data (JSONB) - Agent configuration                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ runs (JSONB ARRAY) - Chat history & execution runs        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ created_at (TIMESTAMP) - Session creation time            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ updated_at (TIMESTAMP) - Last modification time           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ user_id (VARCHAR) - Optional user identification          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                               ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow Diagrams

#### 1. Chat Message Flow (Streaming)
```
User Types Message
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Frontend Action  ‚îÇ
‚îÇ  (sendMessage)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ 1. Add user message to Redux (optimistic)
         ‚îÇ 2. Add empty assistant message placeholder
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  POST /chat/stream   ‚îÇ
‚îÇ  {                   ‚îÇ
‚îÇ    message: "...",   ‚îÇ
‚îÇ    conversation_id   ‚îÇ
‚îÇ  }                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend Agent       ‚îÇ
‚îÇ  1. Load history     ‚îÇ
‚îÇ  2. Create agent     ‚îÇ
‚îÇ  3. Stream tokens    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Ollama Model        ‚îÇ
‚îÇ  Generate response   ‚îÇ
‚îÇ  token by token      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ SSE Stream
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  data: {"delta":"Hi"}‚îÇ
‚îÇ  data: {"delta":" "}‚îÇ
‚îÇ  data: {"delta":"!"}‚îÇ
‚îÇ  data: {"done":true} ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Frontend Updates    ‚îÇ
‚îÇ  1. Append each delta‚îÇ
‚îÇ  2. Update Redux     ‚îÇ
‚îÇ  3. Re-render UI     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  On Stream Complete: ‚îÇ
‚îÇ  - Save title (if 1st)‚îÇ
‚îÇ  - Mark streaming off ‚îÇ
‚îÇ  - Enable input      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 2. Conversation Loading Flow
```
App Loads
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ GET /conversations   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PostgresDb.get_sessions()    ‚îÇ
‚îÇ (SessionType.AGENT filter)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Process Sessions:            ‚îÇ
‚îÇ - Extract titles from        ‚îÇ
‚îÇ   session_data["name"] or    ‚îÇ
‚îÇ   first user message         ‚îÇ
‚îÇ - Count non-system messages  ‚îÇ
‚îÇ - Convert timestamps         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Return Summary Array         ‚îÇ
‚îÇ [{                           ‚îÇ
‚îÇ   conversation_id,           ‚îÇ
‚îÇ   title,                     ‚îÇ
‚îÇ   message_count,             ‚îÇ
‚îÇ   created_at,                ‚îÇ
‚îÇ   updated_at                 ‚îÇ
‚îÇ }]                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Frontend Redux Update        ‚îÇ
‚îÇ setConversations()           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Display in Sidebar           ‚îÇ
‚îÇ with relative timestamps     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

User Clicks Conversation
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Clear current messages    ‚îÇ
‚îÇ 2. Show loading state        ‚îÇ
‚îÇ 3. GET /conversations/{id}   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Load Full Conversation:      ‚îÇ
‚îÇ - Get session from DB        ‚îÇ
‚îÇ - Extract chat_history       ‚îÇ
‚îÇ - Filter system messages     ‚îÇ
‚îÇ - Convert to frontend format ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Add messages to Redux        ‚îÇ
‚îÇ (forEach addMessage)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Display messages in ChatArea ‚îÇ
‚îÇ Hide loading state           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 3. Title Auto-Save Flow
```
First Message Sent
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Stream Completes             ‚îÇ
‚îÇ (done: true event)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Check: Is title "New Chat"?  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ YES
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Generate title:              ‚îÇ
‚îÇ message.slice(0,50) + "..."  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PATCH /conversations/{id}/   ‚îÇ
‚îÇ        title                 ‚îÇ
‚îÇ { title: "..." }             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Backend:                     ‚îÇ
‚îÇ session.session_data["name"] ‚îÇ
‚îÇ   = title                    ‚îÇ
‚îÇ db.upsert_session(session)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Frontend:                    ‚îÇ
‚îÇ updateConversationTitle()    ‚îÇ
‚îÇ Redux state updated          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Sidebar shows new title      ‚îÇ
‚îÇ Persists across page refresh ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üõ†Ô∏è Tech Stack

### Backend Stack
| Technology | Version | Purpose |
|-----------|---------|---------|
| **[FastAPI](https://fastapi.tiangolo.com/)** | 0.115+ | Modern, fast web framework with automatic API docs |
| **[Agno](https://agno.com/)** | Latest | AI agent orchestration with native PostgreSQL support |
| **[Ollama](https://ollama.com/)** | Latest | Local LLM inference engine (privacy-first) |
| **[PostgreSQL](https://www.postgresql.org/)** | 16+ | Robust relational database |
| **[Neon](https://neon.tech/)** | - | Serverless PostgreSQL platform |
| **[psycopg](https://www.psycopg.org/)** | 3.x | PostgreSQL adapter for Python (async support) |
| **[Pydantic](https://pydantic.dev/)** | 2.x | Data validation using Python type annotations |
| **[Uvicorn](https://www.uvicorn.org/)** | Latest | Lightning-fast ASGI server |

### Frontend Stack
| Technology | Version | Purpose |
|-----------|---------|---------|
| **[React](https://react.dev/)** | 19+ | Component-based UI library |
| **[TypeScript](https://www.typescriptlang.org/)** | 5.6+ | Type-safe JavaScript |
| **[Vite](https://vitejs.dev/)** | 7.x | Next generation frontend tooling |
| **[Redux Toolkit](https://redux-toolkit.js.org/)** | 2.x | Official Redux toolset (simplified) |
| **[RTK Query](https://redux-toolkit.js.org/rtk-query/overview)** | - | Powerful data fetching & caching |
| **[shadcn/ui](https://ui.shadcn.com/)** | Latest | Re-usable components (Radix UI based) |
| **[Tailwind CSS](https://tailwindcss.com/)** | 4.x | Utility-first CSS framework |
| **[Lucide React](https://lucide.dev/)** | Latest | Beautiful & consistent icon library |
| **[React Router](https://reactrouter.com/)** | 7.x | Declarative routing for React |
| **[React Markdown](https://github.com/remarkjs/react-markdown)** | 10.x | Markdown rendering with React components |
| **[Marked](https://marked.js.org/)** | 17.x | Fast markdown parser for block-level parsing |
| **[rehype-highlight](https://github.com/rehypejs/rehype-highlight)** | 7.x | Syntax highlighting for code blocks |
| **[remark-gfm](https://github.com/remarkjs/remark-gfm)** | 4.x | GitHub Flavored Markdown support |

### DevOps & Development Tools
| Tool | Purpose |
|------|---------|
| **Git / GitHub** | Version control & collaboration |
| **npm** | Frontend package management |
| **pip / venv** | Python package & environment management |
| **ESLint** | JavaScript/TypeScript linting |
| **Ruff** | Fast Python linter |
| **pytest** | Python testing framework |

## üìã Prerequisites

### Required Software

1. **Python 3.11 or higher**
   ```bash
   # Verify installation
   python --version  # Should show 3.11.x or higher

   # Or use python3 on some systems
   python3 --version
   ```

2. **Node.js 20+ and npm**
   ```bash
   # Verify installation
   node --version  # Should show v20.x.x or higher
   npm --version   # Should show 10.x.x or higher
   ```

   Download from: https://nodejs.org/

3. **Ollama** (Local LLM Runtime)
   ```bash
   # macOS (using Homebrew)
   brew install ollama

   # Linux
   curl -fsSL https://ollama.com/install.sh | sh

   # Windows - Download from https://ollama.com/download
   ```

   Verify installation:
   ```bash
   ollama --version
   ```

4. **PostgreSQL Database** (Neon Account)
   - Sign up at [Neon](https://neon.tech/) (free tier available)
   - Create a new project
   - Copy your connection string (looks like: `postgresql://user:pass@host/db`)

### Optional but Recommended
- **Git** - For version control
- **Docker** - For containerized deployment (future)
- **Make** - Build automation (backend includes Makefile)
- **VS Code** - Recommended IDE with extensions:
  - Python
  - ESLint
  - Prettier
  - Tailwind CSS IntelliSense

## üöÄ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/roeiex74/agno-ollama-chatbot.git
cd agno-ollama-chatbot
```

### 2. Backend Setup

```bash
# Navigate to backend directory
cd backend

# Create and activate virtual environment
python -m venv venv

# Activate (choose based on your OS)
# macOS/Linux:
source venv/bin/activate

# Windows PowerShell:
venv\Scripts\Activate.ps1

# Windows CMD:
venv\Scripts\activate.bat

# Install dependencies
pip install -r requirements.txt
```

### 3. Configure Backend Environment

Create a `.env` file in the `backend/` directory:

```bash
# backend/.env

# Environment (local|prod)
ENV=local

# Ollama configuration
# Recommended models: llama3.2:1b (fast), llama3.2:3b (balanced), llama3.3:70b (best)
OLLAMA_MODEL=llama3.2:3b
OLLAMA_HOST=http://localhost:11434
MODEL_TIMEOUT_S=60

# Database configuration (REPLACE WITH YOUR NEON CONNECTION STRING)
POSTGRES_URL=postgresql+psycopg://user:password@host.region.aws.neon.tech/db?sslmode=require
MAX_HISTORY=20

# Server configuration
HOST=0.0.0.0
PORT=8000
```

**Important**: Replace the `POSTGRES_URL` with your actual Neon connection string!

### 4. Start Ollama and Download Model

```bash
# Start Ollama service (in a separate terminal)
ollama serve

# Pull the model (in another terminal)
ollama pull llama3.2:3b

# Verify model is downloaded
ollama list
# You should see llama3.2:3b in the list
```

### 5. Start Backend Server

```bash
# Make sure you're in the backend directory with venv activated
cd backend
source venv/bin/activate  # If not already activated

# Start the server
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Alternative: use make command (if Make is installed)
make dev
```

You should see:
```
INFO:     Uvicorn running on http://0.0.0.0:8000
INFO:     Application startup complete.
```

**Verify backend is working**:
```bash
curl http://localhost:8000/healthz
```

Expected response:
```json
{
  "status": "ok",
  "environment": "local",
  "model": "llama3.2:3b",
  "database": "postgresql"
}
```

### 6. Frontend Setup

Open a **new terminal** (keep backend running):

```bash
# Navigate to frontend directory
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

You should see:
```
VITE v6.0.x  ready in xxx ms

‚ûú  Local:   http://localhost:5173/
‚ûú  Network: http://192.168.x.x:5173/
```

### 7. Access the Application

Open your browser and navigate to:
```
http://localhost:5173
```

You should see the chat interface! üéâ

### 8. Test the Application

1. Click **"New Chat"** button
2. Type a message (e.g., "Hello! How are you?")
3. Press Enter or click the send button
4. Watch the streaming response appear token-by-token
5. Refresh the page - your conversation should persist
6. Check the sidebar - your conversation should appear with the auto-generated title

## üìÅ Project Structure

```
agno-ollama-chatbot/
‚îÇ
‚îú‚îÄ‚îÄ backend/                         # FastAPI Backend Application
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # FastAPI app, endpoints, lifespan management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                # Environment-based configuration (Pydantic)
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ chatbot_agent.py     # Agno agent with PostgreSQL integration
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ .env                         # Environment variables (not in git)
‚îÇ   ‚îú‚îÄ‚îÄ .env.example                 # Example environment file
‚îÇ   ‚îú‚îÄ‚îÄ Makefile                     # Build automation scripts
‚îÇ   ‚îî‚îÄ‚îÄ pyproject.toml               # Python project metadata
‚îÇ
‚îú‚îÄ‚îÄ frontend/                        # React + Vite Frontend
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/              # React Components
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatArea.tsx         # Message display with scrolling
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInput.tsx        # Message input with auto-resize
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatLayout.tsx       # Main layout (sidebar + chat)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatMessage.tsx      # Individual message component
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ConversationList.tsx # Sidebar conversation list
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memoized-markdown.tsx # Performance-optimized markdown renderer
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ui/                  # shadcn/ui components
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ button.tsx
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ card.tsx
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ input.tsx
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ scroll-area.tsx
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ separator.tsx
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ sheet.tsx
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ skeleton.tsx
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ textarea.tsx
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ tooltip.tsx
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/                   # Custom React Hooks
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useStreamingChat.ts  # SSE streaming logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ use-mobile.ts
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ store/                   # Redux Store
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/                 # RTK Query API Definitions
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ conversationsApi.ts # Conversation CRUD & streaming
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ slices/              # Redux Slices
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversationsSlice.ts # Conversation state
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ uiSlice.ts       # UI state (loading, errors)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks.ts             # Typed Redux hooks
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ store.ts             # Store configuration
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api.ts               # TypeScript type definitions
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api.ts               # API configuration (base URL)
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ conversations.ts     # Data models & helpers
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.ts             # Utility functions (cn)
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx                  # Main app component & routing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tsx                 # App entry point
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.css                # Global styles (Tailwind)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vite.svg                 # Favicon
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ index.html                   # HTML entry point
‚îÇ   ‚îú‚îÄ‚îÄ package.json                 # npm dependencies & scripts
‚îÇ   ‚îú‚îÄ‚îÄ package-lock.json            # Locked dependency versions
‚îÇ   ‚îú‚îÄ‚îÄ vite.config.ts               # Vite configuration
‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.json                # TypeScript configuration
‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.app.json            # App-specific TS config
‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.node.json           # Node-specific TS config
‚îÇ   ‚îú‚îÄ‚îÄ tailwind.config.js           # Tailwind CSS configuration
‚îÇ   ‚îú‚îÄ‚îÄ components.json              # shadcn/ui configuration
‚îÇ   ‚îî‚îÄ‚îÄ eslint.config.js             # ESLint configuration
‚îÇ
‚îú‚îÄ‚îÄ docs/                            # Additional Documentation
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md              # Detailed architecture guide
‚îÇ   ‚îú‚îÄ‚îÄ API_DOCUMENTATION.md         # Complete API reference
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md                # Deployment guide
‚îÇ   ‚îú‚îÄ‚îÄ DEVELOPMENT.md               # Development best practices
‚îÇ   ‚îú‚îÄ‚îÄ TROUBLESHOOTING.md           # Common issues & solutions
‚îÇ   ‚îî‚îÄ‚îÄ AI_DEVELOPMENT.md            # AI development journey
‚îÇ
‚îú‚îÄ‚îÄ .gitignore                       # Git ignore rules
‚îú‚îÄ‚îÄ README.md                        # This file
‚îú‚îÄ‚îÄ CHANGELOG.md                     # Version history
‚îú‚îÄ‚îÄ CONTRIBUTING.md                  # Contribution guidelines
‚îú‚îÄ‚îÄ LICENSE                          # MIT License
‚îú‚îÄ‚îÄ CLAUDE.md                        # Claude Code guidance
‚îî‚îÄ‚îÄ package.json                     # Root package.json (workspaces)
```

### Key Files Explained

#### Backend
- **`app/main.py`**: FastAPI application with all HTTP endpoints and SSE streaming
- **`app/config.py`**: Pydantic-based configuration loading from environment variables
- **`app/agents/chatbot_agent.py`**: Agno agent wrapper with streaming and session management

#### Frontend
- **`src/App.tsx`**: Main application component with routing and top-level state
- **`src/hooks/useStreamingChat.ts`**: Custom hook handling SSE streaming and message updates
- **`src/store/api/conversationsApi.ts`**: RTK Query API for conversation CRUD operations and streaming helper
- **`src/components/ChatLayout.tsx`**: Main layout orchestrating sidebar and chat area
- **`src/components/memoized-markdown.tsx`**: Performance-optimized markdown renderer with block-level memoization

## ‚öôÔ∏è Configuration

### Backend Configuration (.env)

All backend configuration is managed through environment variables:

| Variable | Default | Description | Example |
|----------|---------|-------------|---------|
| `ENV` | `local` | Environment type | `local`, `prod` |
| `OLLAMA_MODEL` | `llama3.2:3b` | Ollama model identifier | `llama3.2:1b`, `llama3.3:70b`, `gpt-oss:20b` |
| `OLLAMA_HOST` | `http://localhost:11434` | Ollama server URL | `http://localhost:11434` |
| `MODEL_TIMEOUT_S` | `60` | Model request timeout (seconds) | `60`, `120` |
| `POSTGRES_URL` | *Required* | PostgreSQL connection string | `postgresql+psycopg://user:pass@host/db` |
| `MAX_HISTORY` | `20` | Max messages per conversation | `10`, `50`, `100` |
| `HOST` | `0.0.0.0` | Server bind host | `0.0.0.0`, `127.0.0.1` |
| `PORT` | `8000` | Server bind port | `8000`, `3000` |

**Switching Models:**

Edit `.env`:
```bash
OLLAMA_MODEL=llama3.3:70b
```

Download the model:
```bash
ollama pull llama3.3:70b
```

Restart the backend server.

### Frontend Configuration

Frontend uses environment variables prefixed with `VITE_`:

Create `frontend/.env.local`:
```bash
# Optional: Override API URL (defaults to http://localhost:8000)
VITE_API_URL=https://your-api-domain.com
```

Configuration is accessed in `frontend/src/config/api.ts`:
```typescript
export const API_CONFIG = {
  BASE_URL: import.meta.env.VITE_API_URL || "http://localhost:8000",
  ENDPOINTS: {
    CHAT: "/chat",
    CHAT_STREAM: "/chat/stream",
    HEALTH: "/healthz",
  },
};
```

## üì° API Documentation

### Base URL
```
http://localhost:8000
```

### Authentication
Currently no authentication required (add in production).

---

### `GET /healthz`

Health check endpoint to verify server status.

**Response:**
```json
{
  "status": "ok",
  "environment": "local",
  "model": "llama3.2:3b",
  "database": "postgresql"
}
```

**Status Codes:**
- `200 OK`: Server is healthy

---

### `POST /chat`

Send a chat message and receive a complete (non-streaming) response.

**Request Body:**
```json
{
  "message": "What is the capital of France?",
  "conversation_id": "conv-123"  // Optional, auto-generated if omitted
}
```

**Response:**
```json
{
  "conversation_id": "conv-123",
  "reply": "The capital of France is Paris.",
  "usage": {
    "model": "llama3.2:3b"
  }
}
```

**Status Codes:**
- `200 OK`: Success
- `503 Service Unavailable`: Agent not initialized
- `500 Internal Server Error`: Processing error

---

### `POST /chat/stream`

Send a chat message and receive a streaming (token-by-token) response via SSE.

**Request Body:**
```json
{
  "message": "Tell me a story",
  "conversation_id": "conv-456"  // Optional
}
```

**Response (Server-Sent Events):**

```
data: {"delta": "Once"}

data: {"delta": " upon"}

data: {"delta": " a"}

data: {"delta": " time"}

data: {"delta": "..."}

data: {"done": true, "conversation_id": "conv-456", "response": "Once upon a time...", "usage": {"model": "llama3.2:3b"}}
```

**Event Format:**
- Each line starts with `data: `
- Each data payload is a JSON object
- Delta events: `{"delta": "token"}`
- Final event: `{"done": true, "conversation_id": "...", "response": "...", "usage": {...}}`

**Headers:**
- `Content-Type: application/json`
- `Accept: text/event-stream`

**Status Codes:**
- `200 OK`: Stream started
- `503 Service Unavailable`: Agent not initialized

---

### `GET /conversations`

List all conversations with summary information.

**Response:**
```json
[
  {
    "conversation_id": "conv-123",
    "title": "What is the capital of France?",
    "message_count": 4,
    "created_at": "2025-11-10T15:47:46",
    "updated_at": "2025-11-10T15:50:18"
  },
  {
    "conversation_id": "conv-456",
    "title": "Tell me a story",
    "message_count": 2,
    "created_at": "2025-11-10T16:00:00",
    "updated_at": "2025-11-10T16:01:30"
  }
]
```

**Notes:**
- System messages are excluded from `message_count`
- Timestamps are in ISO 8601 format
- Ordered by `updated_at` (most recent first)

**Status Codes:**
- `200 OK`: Success
- `503 Service Unavailable`: Agent not initialized
- `500 Internal Server Error`: Database error

---

### `GET /conversations/{conversation_id}`

Get a specific conversation with full message history.

**URL Parameters:**
- `conversation_id` (string): The conversation ID

**Response:**
```json
{
  "conversation_id": "conv-123",
  "title": "What is the capital of France?",
  "messages": [
    {
      "role": "user",
      "content": "What is the capital of France?"
    },
    {
      "role": "assistant",
      "content": "The capital of France is Paris."
    },
    {
      "role": "user",
      "content": "What about Spain?"
    },
    {
      "role": "assistant",
      "content": "The capital of Spain is Madrid."
    }
  ],
  "created_at": "2025-11-10T15:47:46",
  "updated_at": "2025-11-10T15:50:18"
}
```

**Notes:**
- System messages are filtered out from `messages` array
- Only `user` and `assistant` messages are returned

**Status Codes:**
- `200 OK`: Success
- `404 Not Found`: Conversation doesn't exist
- `503 Service Unavailable`: Agent not initialized
- `500 Internal Server Error`: Database error

---

### `DELETE /conversations/{conversation_id}`

Delete a conversation permanently.

**URL Parameters:**
- `conversation_id` (string): The conversation ID to delete

**Response:**
```json
{
  "status": "success",
  "conversation_id": "conv-123"
}
```

**Status Codes:**
- `200 OK`: Successfully deleted
- `503 Service Unavailable`: Agent not initialized
- `500 Internal Server Error`: Deletion failed

---

### `PATCH /conversations/{conversation_id}/title`

Update the title of a conversation.

**URL Parameters:**
- `conversation_id` (string): The conversation ID

**Request Body:**
```json
{
  "title": "My Important Conversation"
}
```

**Response:**
```json
{
  "status": "success",
  "conversation_id": "conv-123",
  "title": "My Important Conversation"
}
```

**Status Codes:**
- `200 OK`: Successfully updated
- `404 Not Found`: Conversation doesn't exist
- `503 Service Unavailable`: Agent not initialized
- `500 Internal Server Error`: Update failed

---

For complete API documentation with examples, see [docs/API_DOCUMENTATION.md](docs/API_DOCUMENTATION.md)

## üíª Development

### Backend Development

```bash
# Activate virtual environment
cd backend
source venv/bin/activate  # macOS/Linux
# or: venv\Scripts\activate  # Windows

# Run with auto-reload (development mode)
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Or use Make (if available)
make dev

# Format code (requires black)
pip install black
black app/

# Lint code (requires ruff)
pip install ruff
ruff check app/

# Type checking (optional)
pip install mypy
mypy app/
```

### Frontend Development

```bash
cd frontend

# Start dev server with HMR (Hot Module Replacement)
npm run dev

# Type checking
npm run type-check

# Lint JavaScript/TypeScript
npm run lint

# Format code (if configured)
npm run format

# Build for production
npm run build

# Preview production build locally
npm run preview
```

### Adding New Features

#### Backend: Add a New Endpoint

1. **Define Pydantic Models** in `app/main.py`:
   ```python
   class MyRequest(BaseModel):
       field: str

   class MyResponse(BaseModel):
       result: str
   ```

2. **Add Endpoint**:
   ```python
   @app.post("/my-endpoint", response_model=MyResponse)
   async def my_endpoint(request: MyRequest) -> MyResponse:
       # Your logic here
       return MyResponse(result="...")
   ```

3. **Add Tests** in `tests/`:
   ```python
   def test_my_endpoint():
       response = client.post("/my-endpoint", json={"field": "value"})
       assert response.status_code == 200
   ```

#### Frontend: Add a New Component

1. **Create Component** in `src/components/`:
   ```typescript
   export function MyComponent({ prop }: { prop: string }) {
     return <div>{prop}</div>;
   }
   ```

2. **Add to Parent Component**:
   ```typescript
   import { MyComponent } from "./components/MyComponent";

   <MyComponent prop="value" />
   ```

3. **Style with Tailwind**:
   ```typescript
   <div className="flex items-center gap-2 p-4 bg-gray-100">
     ...
   </div>
   ```

#### Adding Redux State

1. **Update Slice** in `src/store/slices/`:
   ```typescript
   reducers: {
     myAction: (state, action: PayloadAction<string>) => {
       state.myField = action.payload;
     }
   }
   ```

2. **Export Action**:
   ```typescript
   export const { myAction } = mySlice.actions;
   ```

3. **Use in Component**:
   ```typescript
   import { useAppDispatch } from "@/store/hooks";
   import { myAction } from "@/store/slices/mySlice";

   const dispatch = useAppDispatch();
   dispatch(myAction("value"));
   ```

See [docs/DEVELOPMENT.md](docs/DEVELOPMENT.md) for comprehensive development guide.

## üö¢ Deployment

### Production Checklist

- [ ] Set `ENV=prod` in backend `.env`
- [ ] Use strong PostgreSQL password
- [ ] Enable CORS restrictions in `app/main.py`
- [ ] Set up HTTPS/TLS certificates
- [ ] Configure rate limiting
- [ ] Set up monitoring and logging
- [ ] Configure backup strategy for database
- [ ] Deploy Ollama on GPU-enabled server
- [ ] Build frontend with `npm run build`
- [ ] Set `VITE_API_URL` to production API URL

### Backend Deployment Options

**Option 1: Railway**
```bash
# Install Railway CLI
npm i -g @railway/cli

# Login and deploy
railway login
railway init
railway up
```

**Option 2: Render**
1. Connect GitHub repository
2. Select "Web Service"
3. Set build command: `pip install -r backend/requirements.txt`
4. Set start command: `cd backend && uvicorn app.main:app --host 0.0.0.0 --port $PORT`
5. Add environment variables

**Option 3: Fly.io**
```bash
# Install Fly CLI
curl -L https://fly.io/install.sh | sh

# Deploy
fly launch
fly deploy
```

### Frontend Deployment Options

**Option 1: Vercel (Recommended)**
```bash
# Install Vercel CLI
npm i -g vercel

# Deploy
cd frontend
vercel
```

**Option 2: Netlify**
```bash
# Build
cd frontend
npm run build

# Drag dist/ folder to Netlify Dashboard
# Or use Netlify CLI
netlify deploy --prod
```

**Option 3: Cloudflare Pages**
1. Connect GitHub repository
2. Set build command: `cd frontend && npm run build`
3. Set publish directory: `frontend/dist`

### Database: Neon PostgreSQL

Already configured for production use:
- Serverless architecture
- Auto-scaling
- Automatic backups
- Free tier available

See [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md) for detailed deployment guide.

## üß™ Testing

### Backend Tests

```bash
cd backend

# Run all tests
pytest

# Run with verbose output
pytest -v

# Run specific test file
pytest tests/test_chat_stream.py -v

# Run with coverage
pytest --cov=app --cov-report=html

# Or use Make
make test
make test-coverage
```

### Frontend Tests

```bash
cd frontend

# Run tests (if configured)
npm test

# Run in watch mode
npm test -- --watch

# Run with coverage
npm test -- --coverage
```

### Manual Testing

**Test Streaming:**
```bash
curl -X POST http://localhost:8000/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"message": "Count to 5"}' \
  --no-buffer
```

**Test Conversation Persistence:**
```bash
# Send message
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hi", "conversation_id": "test-123"}'

# List conversations
curl http://localhost:8000/conversations

# Get specific conversation
curl http://localhost:8000/conversations/test-123
```

## üêõ Troubleshooting

### Common Backend Issues

**Issue: Ollama connection failed**
```
Error: Could not connect to Ollama at http://localhost:11434
```
**Solution:**
```bash
# Start Ollama service
ollama serve

# Verify it's running
curl http://localhost:11434/api/version
```

---

**Issue: Database connection error**
```
Error: password authentication failed for user
```
**Solution:**
- Check `POSTGRES_URL` in `.env` is correct
- Verify password and hostname
- Ensure `?sslmode=require` is in connection string
- Test connection string with `psql`:
  ```bash
  psql "postgresql://user:pass@host/db?sslmode=require"
  ```

---

**Issue: Model not found**
```
Error: model 'llama3.2:3b' not found
```
**Solution:**
```bash
# Download the model
ollama pull llama3.2:3b

# Verify it's downloaded
ollama list
```

---

**Issue: Port already in use**
```
ERROR: [Errno 48] Address already in use
```
**Solution:**
```bash
# Find process using port 8000
lsof -ti:8000

# Kill the process
kill -9 $(lsof -ti:8000)

# Or change port in .env
PORT=8001
```

### Common Frontend Issues

**Issue: Can't connect to backend**
```
Failed to fetch from http://localhost:8000
```
**Solution:**
- Verify backend is running: `curl http://localhost:8000/healthz`
- Check CORS is enabled in `app/main.py`
- Clear browser cache
- Check `VITE_API_URL` in frontend config

---

**Issue: Type errors after changes**
```
Type 'X' is not assignable to type 'Y'
```
**Solution:**
```bash
# Restart TypeScript server in VS Code
# Cmd+Shift+P -> "TypeScript: Restart TS Server"

# Or rebuild
npm run build
```

---

**Issue: Blank page after build**
```
Page shows nothing in production
```
**Solution:**
- Check browser console for errors
- Verify `VITE_API_URL` is set correctly
- Check base path in `vite.config.ts`
- Ensure all assets are loading

### Performance Issues

**Slow responses:**
- Use a smaller model: `llama3.2:1b`
- Increase `MODEL_TIMEOUT_S`
- Check Ollama GPU usage: `nvidia-smi` (if NVIDIA GPU)

**High memory usage:**
- Reduce `MAX_HISTORY`
- Use smaller model
- Monitor with: `htop` or `Activity Monitor`

See [docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) for comprehensive troubleshooting guide.

## ü§ñ AI Development Journey

### How This Project Was Built

This entire project was developed using **Claude Code**, Anthropic's AI coding assistant. Every line of code, architectural decision, and documentation was created through iterative collaboration between the developer and Claude.

**Development Process:**

1. **Initial Setup**: Backend structure with Agno and Ollama integration
2. **Database Migration**: Transitioned from SQLite to PostgreSQL (Neon)
3. **Frontend Development**: Built modern React interface with TypeScript
4. **State Management**: Implemented Redux Toolkit with RTK Query
5. **Feature Addition**: Conversation management, streaming, persistence
6. **Debugging**: Fixed serialization issues, infinite loops, type errors
7. **Documentation**: Comprehensive documentation at every step

**Key AI Contributions:**

- ‚úÖ **Code Generation**: 100% of backend and frontend code
- ‚úÖ **Architecture Design**: System design and component structure
- ‚úÖ **Debugging**: Identified and fixed bugs with root cause analysis
- ‚úÖ **Best Practices**: Applied modern development patterns
- ‚úÖ **Documentation**: Created comprehensive docs and comments
- ‚úÖ **Testing**: Designed test strategies and validation

**Technologies Chosen by AI:**

- FastAPI for backend (async, type-safe, auto-docs)
- Agno for agent framework (native PostgreSQL support)
- React + TypeScript for frontend (type safety)
- Redux Toolkit for state management (modern Redux)
- Tailwind CSS for styling (utility-first)
- shadcn/ui for components (accessible, customizable)

This project demonstrates that with proper AI assistance, complex full-stack applications can be built efficiently while maintaining high code quality and best practices.

See [docs/AI_DEVELOPMENT.md](docs/AI_DEVELOPMENT.md) for detailed AI development journey.

## ü§ù Contributing

Contributions are welcome! Whether you're fixing bugs, adding features, or improving documentation.

### How to Contribute

1. **Fork the repository**
   ```bash
   # Click "Fork" on GitHub
   ```

2. **Clone your fork**
   ```bash
   git clone https://github.com/YOUR_USERNAME/agno-ollama-chatbot.git
   cd agno-ollama-chatbot
   ```

3. **Create a feature branch**
   ```bash
   git checkout -b feature/amazing-feature
   ```

4. **Make your changes**
   - Write clear, commented code
   - Follow existing code style
   - Add tests for new features
   - Update documentation

5. **Test your changes**
   ```bash
   # Backend
   cd backend
   pytest

   # Frontend
   cd frontend
   npm run type-check
   npm run lint
   ```

6. **Commit your changes**
   ```bash
   git add .
   git commit -m "Add amazing feature"
   ```

7. **Push to your fork**
   ```bash
   git push origin feature/amazing-feature
   ```

8. **Open a Pull Request**
   - Go to the original repository
   - Click "New Pull Request"
   - Describe your changes
   - Link related issues

### Development Guidelines

- **Code Style**: Follow existing patterns and conventions
- **Commits**: Write clear, descriptive commit messages
- **Documentation**: Update README and docs for significant changes
- **Tests**: Add tests for new features
- **TypeScript**: Maintain type safety in frontend
- **Python**: Use type hints and Pydantic models

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

## üìù License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2025 Agno Ollama Chatbot Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## üôè Acknowledgments

### Technologies & Frameworks
- **[Agno](https://agno.com/)** - Excellent agent framework with first-class PostgreSQL support
- **[Ollama](https://ollama.com/)** - Making local LLM inference accessible to everyone
- **[FastAPI](https://fastapi.tiangolo.com/)** - Modern, fast web framework for Python
- **[React](https://react.dev/)** - Powerful UI library
- **[Neon](https://neon.tech/)** - Serverless PostgreSQL platform
- **[shadcn/ui](https://ui.shadcn.com/)** - Beautiful, accessible component library
- **[Tailwind CSS](https://tailwindcss.com/)** - Utility-first CSS framework
- **[Redux Toolkit](https://redux-toolkit.js.org/)** - Modern Redux made simple

### Development Tools
- **[Claude Code](https://claude.ai/code)** - AI coding assistant that developed this entire project
- **[Anthropic](https://www.anthropic.com/)** - For creating Claude and advancing AI safety

### Community
- Open source community for inspiration and tools
- GitHub for hosting and collaboration
- All contributors who will improve this project

## üìö Additional Documentation

Explore comprehensive documentation:

- **[Architecture Guide](docs/ARCHITECTURE.md)** - Detailed system architecture and design decisions
- **[API Documentation](docs/API_DOCUMENTATION.md)** - Complete API reference with examples
- **[Development Guide](docs/DEVELOPMENT.md)** - Development best practices and guidelines
- **[Deployment Guide](docs/DEPLOYMENT.md)** - Production deployment strategies
- **[Troubleshooting Guide](docs/TROUBLESHOOTING.md)** - Solutions to common problems
- **[AI Development Journey](docs/AI_DEVELOPMENT.md)** - How AI built this project
- **[Changelog](CHANGELOG.md)** - Version history and release notes
- **[Contributing Guidelines](CONTRIBUTING.md)** - How to contribute

## üìû Support & Community

### Getting Help

- üêõ **[Report a Bug](https://github.com/roeiex74/agno-ollama-chatbot/issues/new?template=bug_report.md)**
- üí° **[Request a Feature](https://github.com/roeiex74/agno-ollama-chatbot/issues/new?template=feature_request.md)**
- üí¨ **[Discussions](https://github.com/roeiex74/agno-ollama-chatbot/discussions)** - Ask questions, share ideas
- üìñ **[Wiki](https://github.com/roeiex74/agno-ollama-chatbot/wiki)** - Additional guides and tutorials

### Stay Updated

- ‚≠ê **Star this repository** to show support
- üëÄ **Watch** for updates and releases
- üîî **Subscribe** to release notifications

## üéØ Roadmap

### Phase 1: Core Features ‚úÖ (Complete)
- [x] FastAPI backend with Agno integration
- [x] PostgreSQL database (Neon)
- [x] Streaming chat with SSE
- [x] React 19 frontend with TypeScript
- [x] Conversation management (CRUD)
- [x] Redux Toolkit state management
- [x] Auto-title generation
- [x] Responsive UI design
- [x] Markdown rendering with syntax highlighting
- [x] Performance-optimized memoized rendering
- [x] Copy button for assistant messages

### Phase 2: Enhancements üöß (Upcoming)
- [ ] User authentication & multi-user support
- [ ] Conversation sharing
- [ ] Export conversations (JSON, Markdown, PDF)
- [ ] Search across conversations
- [ ] Conversation folders/tags
- [ ] Dark/light theme toggle
- [ ] Keyboard shortcuts
- [ ] Voice input support

### Phase 3: Advanced Features üîÆ (Future)
- [ ] Multi-agent workflows (Agno Teams)
- [ ] Tool integration (web search, calculator, etc.)
- [ ] File upload and analysis
- [ ] Code execution in sandbox
- [ ] Custom system prompts
- [ ] Model switching in UI
- [ ] Conversation branching
- [ ] Analytics dashboard

### Phase 4: Scale & Performance üöÄ (Future)
- [ ] Redis caching
- [ ] WebSocket support
- [ ] Horizontal scaling
- [ ] Rate limiting
- [ ] Monitoring & observability
- [ ] Load balancing
- [ ] CDN integration

---

<div align="center">

**Built with ‚ù§Ô∏è using Claude Code**

[üè† Home](https://github.com/roeiex74/agno-ollama-chatbot) ‚Ä¢ [üìñ Docs](docs/) ‚Ä¢ [üêõ Issues](https://github.com/roeiex74/agno-ollama-chatbot/issues) ‚Ä¢ [üí¨ Discussions](https://github.com/roeiex74/agno-ollama-chatbot/discussions)

**Star ‚≠ê this repository if you find it helpful!**

</div>
