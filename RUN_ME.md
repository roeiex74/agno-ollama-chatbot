# ğŸš€ ONE COMMAND TO TEST EVERYTHING

No manual steps needed. Everything is fully automated!

## âš¡ Quick Start (Single Command)

```bash
cd backend
make test-all
```

That's it! âœ…

---

## ğŸ“Š What Happens Automatically

When you run `make test-all`, the script will:

1. âœ… **Check Ollama** - Verifies installation
2. âœ… **Start Ollama** - Launches service if not running
3. âœ… **Download Model** - Gets llama3.2:3b if missing (first time only)
4. âœ… **Test Model** - Verifies it works
5. âœ… **Start Server** - Launches FastAPI in background
6. âœ… **Wait for Ready** - Ensures server is up
7. âœ… **Run API Tests** - Tests all endpoints
8. âœ… **Run Unit Tests** - Runs pytest suite
9. âœ… **Show Results** - Displays summary
10. âœ… **Cleanup** - Stops server automatically

**Zero manual intervention required!**

---

## ğŸ“º Expected Output

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  FULLY AUTOMATED END-TO-END TEST
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”â”â” STEP 1: Checking Ollama Installation â”â”â”
âœ“ Ollama is installed

â”â”â” STEP 2: Starting Ollama Service â”â”â”
âœ“ Ollama service started

â”â”â” STEP 3: Checking Model Availability â”â”â”
â„¹ Target model: llama3.2:3b
âœ“ Model 'llama3.2:3b' is available

â”â”â” STEP 4: Testing Model â”â”â”
â„¹ Running quick model test...
âœ“ Model is working: OK

â”â”â” STEP 5: Starting FastAPI Server â”â”â”
â„¹ Launching server in background...
â„¹ Server started with PID: 12345
â„¹ Waiting for server to be ready...
âœ“ Server is ready!

â”â”â” STEP 6: Running API Tests â”â”â”

[TEST 1] Health endpoint...
âœ“ Health check PASSED

[TEST 2] Non-streaming chat...
âœ“ Chat endpoint PASSED

[TEST 3] Conversation memory...
âœ“ Memory PASSED

[TEST 4] Streaming chat...
âœ“ Streaming PASSED

â”â”â” STEP 7: Running Unit Tests â”â”â”
â„¹ Executing pytest...
test_health.py::test_health_check PASSED
test_chat_nonstream.py::test_chat_endpoint_success PASSED
test_chat_stream.py::test_chat_stream_endpoint PASSED
test_memory.py::test_inmemory_store_load_empty PASSED
...
âœ“ Unit tests PASSED

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ ALL TESTS PASSED!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Success Summary:
  âœ“ Ollama service running
  âœ“ Model loaded and tested
  âœ“ FastAPI server running
  âœ“ All API endpoints working
  âœ“ Unit tests passing
```

---

## â±ï¸ How Long Does It Take?

- **First run:** 10-15 minutes (downloads ~2GB model)
- **Subsequent runs:** 1-2 minutes

---

## ğŸ”§ Prerequisites

Only **Python 3.10+** is required. Everything else is handled automatically!

If Ollama is not installed, the script will tell you:
```
âœ— Ollama is not installed!

Please install Ollama:
  macOS/Linux: curl -fsSL https://ollama.com/install.sh | sh
```

Install it, then run `make test-all` again.

---

## ğŸ¯ Other Useful Commands

```bash
# One-time setup (if not done yet)
make setup

# Just start the server (manual mode)
make start

# Run only unit tests (fast)
make test

# Check Ollama status
make check-ollama
```

---

## ğŸ› If Something Fails

The script will show exactly what failed and where to find logs:

```
Logs available at:
  - Server logs: /tmp/fastapi.log
  - Ollama logs: /tmp/ollama.log
  - Pytest logs: /tmp/pytest.log
```

Common fixes:

**Port already in use:**
```bash
# Kill any process on port 8000
lsof -ti:8000 | xargs kill -9

# Run again
make test-all
```

**Ollama issues:**
```bash
# Check Ollama status
ollama list

# Restart Ollama
pkill ollama
ollama serve &

# Run again
make test-all
```

---

## âœ… Success Checklist

After running `make test-all`, you should see:

- [x] âœ“ Ollama service running
- [x] âœ“ Model loaded and tested
- [x] âœ“ FastAPI server running
- [x] âœ“ All API endpoints working
- [x] âœ“ Unit tests passing

---

## ğŸ¬ Ready?

Just run:

```bash
cd backend
make test-all
```

Sit back and watch it work! â˜•

---

## ğŸš€ For Development

If you want the server to **keep running** after tests:

```bash
# Start server (stays running)
make start

# In another terminal, run tests
make test-api
```

---

**That's it! Everything is automated.** ğŸ‰
